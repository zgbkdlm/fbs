{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try train a Schrodinger bridge between a Gaussian mixture and a unit Gaussian.\n",
    "\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from fbs.sdes import make_linear_sde, StationaryConstLinearSDE\n",
    "from fbs.dsb.base import ipf_loss_disc\n",
    "from fbs.nn.models import make_st_nn, GMSBMLP\n",
    "from fbs.nn.utils import make_optax_kernel\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def posterior_sampler(key_, y_):\n",
    "    key_choice, key_g1, key_g2 = jax.random.split(key_, num=3)\n",
    "    std_1 = 0.5\n",
    "    std_2 = 0.5\n",
    "    g1 = 0.5 * (y_ + std_1 * jax.random.normal(key_g1, (1,)))\n",
    "    g2 = 0.5 * (-y_ + std_2 * jax.random.normal(key_g2, (1,)))\n",
    "    return jax.random.choice(key_choice, jnp.vstack([g1, g2]), axis=0)\n",
    "\n",
    "\n",
    "def data_sampler(key_):\n",
    "    key_y, key_cond = jax.random.split(key_, num=2)\n",
    "    y_ = 0.5 * jax.random.normal(key_y, (1,))\n",
    "    return jnp.concatenate([posterior_sampler(key_cond, y_), y_], axis=-1)\n",
    "\n",
    "\n",
    "def ref_sampler(key_):\n",
    "    return jax.random.normal(key_, (2,))\n",
    "\n",
    "\n",
    "nsamples = 10000\n",
    "key = jax.random.PRNGKey(666)\n",
    "keys = jax.random.split(key, nsamples)\n",
    "\n",
    "y = jnp.array([1.])\n",
    "samples = jax.vmap(posterior_sampler, in_axes=[0, None])(keys, y)\n",
    "\n",
    "plt.hist(samples[:, 0], density=True, bins=100, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot the data and ref samples\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, nsamples)\n",
    "data_samples = jax.vmap(data_sampler)(keys)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, nsamples)\n",
    "ref_samples = jax.vmap(ref_sampler)(keys)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, sharey='row')\n",
    "axes[0].scatter(data_samples[:, 0], data_samples[:, 1], s=1, alpha=0.7)\n",
    "axes[1].scatter(ref_samples[:, 0], ref_samples[:, 1], s=1, alpha=0.7)\n",
    "plt.tight_layout(pad=0.1)\n",
    "plt.show()\n",
    "\n",
    "# SB settings\n",
    "nsbs = 10  # number of SB iterations\n",
    "nsteps = 50\n",
    "ks = jnp.arange(nsteps + 1)\n",
    "T = 0.5\n",
    "dt = T / nsteps\n",
    "ts = jnp.linspace(0., T, nsteps + 1)\n",
    "\n",
    "sde = StationaryConstLinearSDE(a=-0.5, b=1.)\n",
    "discretise_linear_sde, _, _ = make_linear_sde(sde)\n",
    "F, Q = discretise_linear_sde(dt, 0.)\n",
    "F = 0.99\n",
    "print(F, Q)\n",
    "\n",
    "# NN setting\n",
    "batch_size = 512\n",
    "key, subkey = jax.random.split(key)\n",
    "nn_fwd = GMSBMLP(dim=2)\n",
    "param_fwd, _, nn_fn_fwd = make_st_nn(subkey, nn=nn_fwd, dim_in=(2,), batch_size=batch_size)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "nn_bwd = GMSBMLP(dim=2)\n",
    "param_bwd, _, nn_fn_bwd = make_st_nn(subkey, nn=nn_bwd, dim_in=(2,), batch_size=batch_size)\n",
    "\n",
    "\n",
    "def simulate_disc(key_, z0s_, ts_, param_, fn):\n",
    "    def scan_body(carry, elem):\n",
    "        z = carry\n",
    "        k, rnd = elem\n",
    "        z = fn(z, k, param_) + jnp.sqrt(dt) * rnd\n",
    "        return z, None\n",
    "\n",
    "    n, d = z0s_.shape\n",
    "    rnds = jax.random.normal(key_, (nsteps, n, d))\n",
    "    return jax.lax.scan(scan_body, z0s_, (ts_[:-1] / dt, rnds))[0]\n",
    "\n",
    "\n",
    "# Optax setting\n",
    "niters = 1000\n",
    "# schedule = optax.cosine_decay_schedule(init_value=1e-2, decay_steps=niters // 10)\n",
    "schedule = optax.constant_schedule(1e-3)\n",
    "# schedule = optax.exponential_decay(1e-2, niters // 100, .96)\n",
    "optimiser = optax.adam(learning_rate=schedule)\n",
    "\n",
    "\n",
    "# optimiser = optax.chain(optax.clip_by_global_norm(1.),\n",
    "#                         optimiser)\n",
    "\n",
    "def init_loss_fn(param_bwd_, param_fwd_, key_):\n",
    "    \"\"\"Simulate the forward data -> sth. to learn its backward.\n",
    "    \"\"\"\n",
    "    key_data, key_loss, key_ts = jax.random.split(key_, num=3)\n",
    "    keys_ = jax.random.split(key_data, num=batch_size)\n",
    "    data_samples = jax.vmap(data_sampler)(keys_)\n",
    "    rnd_ts = jnp.hstack([0.,\n",
    "                         jnp.sort(jax.random.uniform(key_ts, (nsteps - 1,), minval=0. + 1e-5, maxval=T)),\n",
    "                         T])\n",
    "    ks = rnd_ts / dt\n",
    "    Qs = jnp.diff(rnd_ts)\n",
    "    return ipf_loss_disc(param_bwd_, param_fwd_, data_samples, ks, Qs, nn_fn_bwd, lambda x, k, p: F * x, key_loss)\n",
    "\n",
    "\n",
    "def bwd_loss_fn(param_bwd_, param_fwd_, key_):\n",
    "    \"\"\"Simulate the forward data -> sth. to learn its backward.\n",
    "    \"\"\"\n",
    "    key_data, key_loss, key_ts = jax.random.split(key_, num=3)\n",
    "    keys_ = jax.random.split(key_data, num=batch_size)\n",
    "    data_samples = jax.vmap(data_sampler)(keys_)\n",
    "    rnd_ts = jnp.hstack([0.,\n",
    "                         jnp.sort(jax.random.uniform(key_ts, (nsteps - 1,), minval=0. + 1e-5, maxval=T)),\n",
    "                         T])\n",
    "    ks = rnd_ts / dt\n",
    "    Qs = jnp.diff(rnd_ts)\n",
    "    return ipf_loss_disc(param_bwd_, param_fwd_, data_samples, ks, Qs, nn_fn_bwd, nn_fn_fwd, key_loss)\n",
    "\n",
    "\n",
    "def fwd_loss_fn(param_fwd_, param_bwd_, key_):\n",
    "    \"\"\"Simulate the backward sth. <- ref to learn its forward.\n",
    "    \"\"\"\n",
    "    key_ref, key_loss, key_ts = jax.random.split(key_, num=3)\n",
    "    keys_ = jax.random.split(key_ref, num=batch_size)\n",
    "    ref_samples = jax.vmap(ref_sampler)(keys_)\n",
    "    rnd_ts = jnp.hstack([0.,\n",
    "                         jnp.sort(jax.random.uniform(key_ts, (nsteps - 1,), minval=0. + 1e-5, maxval=T)),\n",
    "                         T])\n",
    "    ks = rnd_ts / dt\n",
    "    Qs = jnp.diff(rnd_ts)\n",
    "    return ipf_loss_disc(param_fwd_, param_bwd_, ref_samples, ks[::-1], Qs[::-1], nn_fn_fwd, nn_fn_bwd, key_loss)\n",
    "\n",
    "\n",
    "optax_kernel_init, _ = make_optax_kernel(optimiser, init_loss_fn, jit=True)\n",
    "optax_kernel_bwd, _ = make_optax_kernel(optimiser, bwd_loss_fn, jit=True)\n",
    "optax_kernel_fwd, _ = make_optax_kernel(optimiser, fwd_loss_fn, jit=True)\n",
    "\n",
    "\n",
    "def sb_kernel(param_fwd_, param_bwd_, key_, sb_step):\n",
    "    # Compute the backward\n",
    "    opt_state = optimiser.init(param_bwd_)\n",
    "    for i in range(niters):\n",
    "        key_, subkey_ = jax.random.split(key_)\n",
    "        if sb_step == 0:\n",
    "            param_bwd_, opt_state, loss = optax_kernel_init(param_bwd_, opt_state, param_fwd_, subkey_)\n",
    "        else:\n",
    "            param_bwd_, opt_state, loss = optax_kernel_bwd(param_bwd_, opt_state, param_fwd_, subkey_)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Learning backward | SB: {sb_step} | iter: {i} | loss: {loss}')\n",
    "\n",
    "    # Compute the forward\n",
    "    opt_state = optimiser.init(param_fwd_)\n",
    "    for i in range(niters):\n",
    "        key_, subkey_ = jax.random.split(key_)\n",
    "        param_fwd_, opt_state, loss = optax_kernel_fwd(param_fwd_, opt_state, param_bwd_, subkey_)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Learning forward | SB: {sb_step} | iter: {i} | loss: {loss}')\n",
    "\n",
    "    return param_fwd_, param_bwd_"
   ],
   "id": "ba598db0d1da6a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SB iterations\n",
    "for j in range(nsbs):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    param_fwd, param_bwd = sb_kernel(param_fwd, param_bwd, subkey, j)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    approx_ref_samples = simulate_disc(subkey, data_samples, ts, param_fwd, nn_fn_fwd)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    approx_data_samples = simulate_disc(subkey, ref_samples, ts[::-1], param_bwd, nn_fn_bwd)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "    axes[0, 0].scatter(data_samples[:, 0], data_samples[:, 1], s=1, alpha=0.7)\n",
    "    axes[0, 1].scatter(approx_data_samples[:, 0], approx_data_samples[:, 1], s=1, alpha=0.7)\n",
    "\n",
    "    axes[1, 0].scatter(ref_samples[:, 0], ref_samples[:, 1], s=1, alpha=0.7)\n",
    "    axes[1, 1].scatter(approx_ref_samples[:, 0], approx_ref_samples[:, 1], s=1, alpha=0.7)\n",
    "\n",
    "    plt.title(f'SB step {j}')\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    plt.show()\n"
   ],
   "id": "ad579c6544219903"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
